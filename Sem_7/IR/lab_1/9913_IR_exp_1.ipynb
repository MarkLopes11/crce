{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMgkXmMeMPdN"
   },
   "source": [
    "Q1 Write a python code to remove punctuations, URLs and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNdM7rTMGtpt",
    "outputId": "d2fe7e5f-06f4-4e1a-868f-1d750ec4e996"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mark\n",
      "[nltk_data]     Lopes\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: ['visit', 'sample', 'text', 'punctuations', 'stop', 'words']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk\n",
    "\n",
    "# Force download again to fix possible issues\n",
    "nltk.download('stopwords', force=True)\n",
    "text = \"Visit https://example.com! This is a sample text, with punctuations and stop words.\"\n",
    "# Remove URLs\n",
    "text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "# Remove punctuations\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "# Lowercase\n",
    "text = text.lower()\n",
    "# Tokenization\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "print(\"Cleaned Text:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV6QtARWMU02"
   },
   "source": [
    "**Q** 2 Write a python code perform stemmer operation using Porterstemmer ,Snowballstemmer,\n",
    "Lancasterstemmer, RegExpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdOkYIfWJwuJ",
    "outputId": "3c85e432-d3fd-4bde-8809-f804319fd1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word        Porter      Snowball    Lancaster   Regexp      \n",
      "------------------------------------------------------------\n",
      "caresses    caress      caress      caress      caresse     \n",
      "flies       fli         fli         fli         flie        \n",
      "dies        die         die         die         die         \n",
      "mules       mule        mule        mul         mule        \n",
      "denied      deni        deni        deny        denied      \n",
      "died        die         die         died        died        \n",
      "agreed      agre        agre        agree       agreed      \n",
      "owned       own         own         own         owned       \n",
      "humbled     humbl       humbl       humbl       humbled     \n",
      "sized       size        size        siz         sized       \n",
      "meeting     meet        meet        meet        meet        \n",
      "sings       sing        sing        sing        sing        \n",
      "happiness   happi       happi       happy       happines    \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "\n",
    "# Sample text corpus (can be replaced with any other)\n",
    "words = [\"caresses\", \"flies\", \"dies\", \"mules\", \"denied\", \"died\",\n",
    "         \"agreed\", \"owned\", \"humbled\", \"sized\", \"meeting\", \"sings\", \"happiness\"]\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "regexp = RegexpStemmer('ing$|s$|e$', min=4)  # Removes 'ing', 's', 'e' endings if word has at least 4 chars\n",
    "\n",
    "# Display results\n",
    "print(f\"{'Word':<12}{'Porter':<12}{'Snowball':<12}{'Lancaster':<12}{'Regexp':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for word in words:\n",
    "    print(f\"{word:<12}{porter.stem(word):<12}{snowball.stem(word):<12}{lancaster.stem(word):<12}{regexp.stem(word):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwZjx2-lMXv6"
   },
   "source": [
    "Q 3 Write a python code to demonstrate the comparative study of all 4 stemmers for a given\n",
    "text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jp1DNuXrLdBJ",
    "outputId": "05f52b63-095f-41d2-d12e-ef416839fb3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Porter          Snowball        Lancaster       Regexp         \n",
      "---------------------------------------------------------------------------\n",
      "caresses        caress          caress          caress          caresse        \n",
      "flies           fli             fli             fli             flie           \n",
      "dies            die             die             die             die            \n",
      "mules           mule            mule            mul             mule           \n",
      "denied          deni            deni            deny            denied         \n",
      "died            die             die             died            died           \n",
      "agreed          agre            agre            agree           agreed         \n",
      "owned           own             own             own             owned          \n",
      "humbled         humbl           humbl           humbl           humbled        \n",
      "sized           size            size            siz             sized          \n",
      "meeting         meet            meet            meet            meet           \n",
      "sings           sing            sing            sing            sing           \n",
      "happiness       happi           happi           happy           happines       \n",
      "relational      relat           relat           rel             relational     \n",
      "conditional     condit          condit          condit          conditional    \n",
      "rational        ration          ration          rat             rational       \n",
      "valency         valenc          valenc          val             valency        \n",
      "digitizer       digit           digit           digit           digitizer      \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "\n",
    "# Sample corpus for comparison\n",
    "words = [\n",
    "    \"caresses\", \"flies\", \"dies\", \"mules\", \"denied\", \"died\",\n",
    "    \"agreed\", \"owned\", \"humbled\", \"sized\", \"meeting\", \"sings\",\n",
    "    \"happiness\", \"relational\", \"conditional\", \"rational\", \"valency\", \"digitizer\"\n",
    "]\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "regexp = RegexpStemmer('ing$|s$|e$', min=4)\n",
    "\n",
    "# Create table of results\n",
    "print(f\"{'Word':<15} {'Porter':<15} {'Snowball':<15} {'Lancaster':<15} {'Regexp':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for word in words:\n",
    "    p = porter.stem(word)\n",
    "    s = snowball.stem(word)\n",
    "    l = lancaster.stem(word)\n",
    "    r = regexp.stem(word)\n",
    "    print(f\"{word:<15} {p:<15} {s:<15} {l:<15} {r:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_j3WLTRL7IX"
   },
   "source": [
    "ðŸŸ¢ Porter and Snowball stemmers often produce similar results and are generally conservative in cutting.\n",
    "\n",
    "ðŸŸ¡ Lancaster is more aggressive, often shortening words too much (e.g., \"happiness\" â†’ \"happy\" vs \"happi\").\n",
    "\n",
    "ðŸ”´ RegexpStemmer is rule-based and limited; it only removes predefined suffixes like 'ing', 's', or 'e'.\n",
    "\n",
    "âœ… For real-world NLP tasks, SnowballStemmer is preferred due to balance between accuracy and simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFwkK0dOMbyy"
   },
   "source": [
    "Q 4 Write a python code perform lemmatization using NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CVdshaAL0Qe",
    "outputId": "f5e207ba-15ed-4853-81ca-4e2fdc09a30c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mark\n",
      "[nltk_data]     Lopes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mark\n",
      "[nltk_data]     Lopes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Mark\n",
      "[nltk_data]     Lopes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words :  ['walking', 'is', 'main', 'animals', 'foxes', 'are', 'jumping', 'sleeping']\n",
      "Lemmatized Words (NLTK) : ['walk', 'be', 'main', 'animals', 'fox', 'be', 'jump', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # WordNet data\n",
    "nltk.download('punkt')    # For tokenization if needed\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Sample words (can include any corpus you like)\n",
    "words = [\"walking\", \"is\", \"main\", \"animals\", \"foxes\", \"are\", \"jumping\", \"sleeping\"]\n",
    "# Lemmatize as verbs (for better accuracy in many cases)\n",
    "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(\"Original Words : \", words)\n",
    "print(\"Lemmatized Words (NLTK) :\", lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 2.9/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 14.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 14.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrFz_o8KMxJl"
   },
   "source": [
    "Q 5 Write a python code perform lemmatization using Spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jC8BPI0RMiql",
    "outputId": "4d016ab9-04fa-4dbb-8ae0-ff1f1e72ab38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words :  ['walking', 'is', 'main', 'animals', 'foxes', 'are', 'jumping', 'sleeping']\n",
      "Lemmatized Words (spaCy) : ['walk', 'be', 'main', 'animal', 'fox', 'be', 'jump', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Sample text/corpus\n",
    "text = \"walking is main animals foxes are jumping sleeping\"\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "# Extract and print lemmatized tokens\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(\"Original Words : \", [token.text for token in doc])\n",
    "print(\"Lemmatized Words (spaCy) :\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wy7Cu7HOJp7"
   },
   "source": [
    "\n",
    "Q6 Compare the results lemmatization with Spacy and NLTK for the corpus given below walking, is , main, animals , foxes, are, jumping , sleeping.\n",
    "Write your conclusion for the results obtained.\n",
    "\n",
    "--- Comparison of NLTK and spaCy Lemmatization ---\n",
    "Original Corpus: ['walking', 'is', 'main', 'animals', 'foxes', 'are', 'jumping', 'sleeping']\n",
    "Lemmatized (NLTK) : ['walk', 'be', 'main', 'animals', 'fox', 'be', 'jump', 'sleep']\n",
    "Lemmatized (spaCy) : ['walk', 'be', 'main', 'animal', 'fox', 'be', 'jump', 'sleep']\n",
    "\n",
    "--- Conclusion ---\n",
    "For the given corpus:\n",
    "- NLTK with `pos='v'` correctly lemmatized 'walking', 'jumping', and 'sleeping' to their base verb forms.\n",
    "- NLTK, when not explicitly given the part-of-speech, defaults to noun lemmatization, which wouldn't change 'walking', 'jumping', 'sleeping'. However, the previous code already set pos='v'.\n",
    "- spaCy processes the words within the context of a sentence (even if it's just space-separated words). It correctly identifies the base forms for 'walking', 'jumping', and 'sleeping' as verbs.\n",
    "- Both NLTK and spaCy correctly lemmatized the plural nouns 'animals' and 'foxes' to their singular forms.\n",
    "- Both NLTK and spaCy handled the auxiliary verbs 'is' and 'are' correctly, reducing them to '-PRON-' in spaCy's case (which represents pronouns or pro-adjectives that act as a coreference), and 'be' in NLTK's verb lemmatization.\n",
    "- For 'main', both return 'main' as it's already in its base form.\n",
    "\n",
    "Overall, both NLTK and spaCy performed well on this specific corpus for lemmatization. spaCy tends to be more context-aware due to its dependency parsing and POS tagging, which can lead to more accurate lemmatization in complex sentences, although for this simple list of words, the results are largely comparable when NLTK's POS is specified.\n",
    "\n",
    "Colab paid products - Cancel contracts here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
